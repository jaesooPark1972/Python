#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ğŸµ ì‹¤ì „ ìŒì„± í›ˆë ¨ ì—”ì§„ (GTX 1060 3GB ìµœì í™”)
================================================
ë°•ì¬ìˆ˜ ë‹˜ì˜ ê²½ëŸ‰ í…œí”Œë¦¿ ê¸°ë°˜ + ì‹¤ì œ ìŒì„± í•™ìŠµ ê¸°ëŠ¥ ì¶”ê°€

Author: Park Jae-soo (SKY Group)
Based on: LightTrainer template
Version: 2.0 (Production Ready)
"""

import os
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torchaudio
import gc
import json
from pathlib import Path

class VoiceEncoder(nn.Module):
    """
    ê²½ëŸ‰ ìŒì„± ì¸ì½”ë” (VRAM 3GB ìµœì í™”)
    ì…ë ¥: Mel-Spectrogram
    ì¶œë ¥: Voice Embedding (256-dim)
    """
    def __init__(self, input_dim=80, hidden_dim=256, output_dim=256):
        super().__init__()
        
        # CNN Layers (íŠ¹ì§• ì¶”ì¶œ)
        self.conv1 = nn.Conv1d(input_dim, 128, kernel_size=3, padding=1)
        self.conv2 = nn.Conv1d(128, 256, kernel_size=3, padding=1)
        self.conv3 = nn.Conv1d(256, hidden_dim, kernel_size=3, padding=1)
        
        # Pooling
        self.pool = nn.AdaptiveAvgPool1d(1)
        
        # Output
        self.fc = nn.Linear(hidden_dim, output_dim)
        
        # Batch Norm (í•™ìŠµ ì•ˆì •í™”)
        self.bn1 = nn.BatchNorm1d(128)
        self.bn2 = nn.BatchNorm1d(256)
        self.bn3 = nn.BatchNorm1d(hidden_dim)
        
    def forward(self, x):
        # x: (batch, mel_bins, time)
        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))
        
        # Global pooling
        x = self.pool(x).squeeze(-1)
        
        # Output embedding
        x = self.fc(x)
        return x


class RealVoiceTrainer:
    """
    ì‹¤ì „ ìŒì„± í›ˆë ¨ ì—”ì§„
    - ì‹¤ì œ WAV íŒŒì¼ ë¡œë“œ
    - Mel-Spectrogram ë³€í™˜
    - ìŒì„± ì„ë² ë”© í•™ìŠµ
    - .pth ëª¨ë¸ ìƒì„±
    """
    
    def __init__(self, device="cuda" if torch.cuda.is_available() else "cpu"):
        self.device = device
        self.is_running = False
        self.model = None
        self.optimizer = None
        
        # Mel-Spectrogram ì„¤ì •
        self.mel_transform = torchaudio.transforms.MelSpectrogram(
            sample_rate=16000,
            n_fft=1024,
            hop_length=256,
            n_mels=80,
            f_min=0,
            f_max=8000
        ).to(device)
        
        print(f"âš¡ ì‹¤ì „ í›ˆë ¨ ì—”ì§„ ì´ˆê¸°í™” ì™„ë£Œ (ì¥ì¹˜: {self.device})")
        
    def clear_memory(self):
        """VRAM ë©”ëª¨ë¦¬ ê°•ì œ ì²­ì†Œ (3GB ìƒì¡´ í•„ìˆ˜)"""
        gc.collect()
        if self.device == "cuda":
            torch.cuda.empty_cache()
            
    def load_audio(self, audio_path, target_sr=16000, max_duration=10.0):
        """
        ì˜¤ë””ì˜¤ íŒŒì¼ ë¡œë“œ ë° ì „ì²˜ë¦¬
        
        Args:
            audio_path: WAV íŒŒì¼ ê²½ë¡œ
            target_sr: ëª©í‘œ ìƒ˜í”Œë ˆì´íŠ¸ (16kHz)
            max_duration: ìµœëŒ€ ê¸¸ì´ (ì´ˆ) - VRAM ë³´í˜¸
        
        Returns:
            waveform: (1, samples) tensor
        """
        try:
            # ì˜¤ë””ì˜¤ ë¡œë“œ
            waveform, sr = torchaudio.load(audio_path)
            
            # ëª¨ë…¸ë¡œ ë³€í™˜
            if waveform.shape[0] > 1:
                waveform = torch.mean(waveform, dim=0, keepdim=True)
            
            # ë¦¬ìƒ˜í”Œë§
            if sr != target_sr:
                resampler = torchaudio.transforms.Resample(sr, target_sr)
                waveform = resampler(waveform)
            
            # ê¸¸ì´ ì œí•œ (VRAM ë³´í˜¸)
            max_samples = int(target_sr * max_duration)
            if waveform.shape[1] > max_samples:
                waveform = waveform[:, :max_samples]
            
            return waveform
            
        except Exception as e:
            print(f"âš ï¸ ì˜¤ë””ì˜¤ ë¡œë“œ ì‹¤íŒ¨ ({audio_path}): {e}")
            return None
    
    def extract_mel_spectrogram(self, waveform):
        """
        Mel-Spectrogram ì¶”ì¶œ
        
        Args:
            waveform: (1, samples) tensor
        
        Returns:
            mel: (1, n_mels, time) tensor
        """
        with torch.no_grad():
            # [FIX] waveformì„ mel_transformê³¼ ê°™ì€ ë””ë°”ì´ìŠ¤ë¡œ ì´ë™
            waveform = waveform.to(self.device)
            mel = self.mel_transform(waveform)
            # Log scale
            mel = torch.log(mel + 1e-9)
        return mel
    
    def train(self, package_path, model_name, epochs=20, progress_callback=None):
        """
        ì‹¤ì œ ìŒì„± í›ˆë ¨ ë©”ì¸ í•¨ìˆ˜
        
        Args:
            package_path: GPT_SoVITS_Training_{timestamp}/ í´ë”
            model_name: ì €ì¥í•  ëª¨ë¸ ì´ë¦„
            epochs: í›ˆë ¨ ë°˜ë³µ íšŸìˆ˜
            progress_callback: GUI ì—…ë°ì´íŠ¸ í•¨ìˆ˜(progress, message)
        
        Returns:
            final_model_path: ìƒì„±ëœ .pth íŒŒì¼ ê²½ë¡œ
        """
        self.is_running = True
        self.clear_memory()
        
        print(f"ğŸš€ [{model_name}] ì‹¤ì „ í›ˆë ¨ ì‹œì‘...")
        print(f"ğŸ“‚ ë°ì´í„°: {package_path}")
        
        try:
            # ========================================
            # 1. ë°ì´í„° ë¡œë“œ
            # ========================================
            audio_dir = os.path.join(package_path, "audio")
            if not os.path.exists(audio_dir):
                raise Exception(f"ì˜¤ë””ì˜¤ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {audio_dir}")
            
            audio_files = [f for f in os.listdir(audio_dir) 
                          if f.endswith(('.wav', '.mp3', '.flac'))]
            
            if len(audio_files) == 0:
                raise Exception("í›ˆë ¨ìš© ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            
            print(f"âœ“ {len(audio_files)}ê°œ ì˜¤ë””ì˜¤ íŒŒì¼ ë°œê²¬")
            
            # ì˜¤ë””ì˜¤ ë¡œë“œ ë° Mel ë³€í™˜ (ì²­í¬ ë‹¨ìœ„ ì²˜ë¦¬)
            mel_spectrograms = []
            
            # [FIX] VRAM ë³´í˜¸ë¥¼ ìœ„í•´ 10ì´ˆ ë‹¨ìœ„ë¡œ ìë¥´ë˜, ì „ì²´ ë°ì´í„°ë¥¼ ëª¨ë‘ ì‚¬ìš©
            CHUNK_DURATION = 10.0 
            
            for i, audio_file in enumerate(audio_files):
                audio_path = os.path.join(audio_dir, audio_file)
                
                try:
                    # 1. ì›ë³¸ ë¡œë“œ (ì „ì²´ ê¸¸ì´)
                    full_waveform, sr = torchaudio.load(audio_path)
                    
                    # ëª¨ë…¸ ë³€í™˜
                    if full_waveform.shape[0] > 1:
                        full_waveform = torch.mean(full_waveform, dim=0, keepdim=True)
                    
                    # ë¦¬ìƒ˜í”Œë§ (16000Hz)
                    if sr != 16000:
                        resampler = torchaudio.transforms.Resample(sr, 16000)
                        full_waveform = resampler(full_waveform)
                    
                    # 2. 10ì´ˆ ë‹¨ìœ„ë¡œ ìŠ¬ë¼ì´ì‹± (Truncation ì œê±°)
                    total_samples = full_waveform.shape[1]
                    samples_per_chunk = int(16000 * CHUNK_DURATION)
                    
                    chunks_created = 0
                    for start in range(0, total_samples, samples_per_chunk):
                        end = start + samples_per_chunk
                        chunk = full_waveform[:, start:end]
                        
                        # ë„ˆë¬´ ì§§ì€ ì²­í¬(1ì´ˆ ë¯¸ë§Œ)ëŠ” ì œì™¸
                        if chunk.shape[1] < 16000: 
                             continue
                             
                        # íŒ¨ë”© (í•„ìš”ì‹œ)
                        if chunk.shape[1] < samples_per_chunk:
                            pad_size = samples_per_chunk - chunk.shape[1]
                            chunk = F.pad(chunk, (0, pad_size))
                        
                        # Mel ë³€í™˜ ë° ë“±ë¡
                        mel = self.extract_mel_spectrogram(chunk)
                        mel_spectrograms.append(mel)
                        chunks_created += 1
                        
                    print(f"   > {audio_file}: {chunks_created}ê°œ êµ¬ê°„ìœ¼ë¡œ ë¶„í• ë¨")

                except Exception as e:
                    print(f"âš ï¸ ì˜¤ë””ì˜¤ ì²˜ë¦¬ ì‹¤íŒ¨ ({audio_file}): {e}")
                
                if progress_callback:
                    load_progress = int((i + 1) / len(audio_files) * 10)
                    progress_callback(load_progress, f"ë°ì´í„° ì²˜ë¦¬ ì¤‘... ({i+1}/{len(audio_files)}) - {len(mel_spectrograms)}ê°œ ìƒ˜í”Œ")
            
            if len(mel_spectrograms) == 0:
                raise Exception("ìœ íš¨í•œ ì˜¤ë””ì˜¤ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤!")
            
            print(f"âœ“ ì´ {len(mel_spectrograms)}ê°œ í•™ìŠµ ë°ì´í„°(Mel) ìƒì„± ì™„ë£Œ")
            
            # ========================================
            # 2. ëª¨ë¸ ì´ˆê¸°í™”
            # ========================================
            self.model = VoiceEncoder().to(self.device)
            self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.001)
            
            # Loss function (Contrastive Learning)
            criterion = nn.TripletMarginLoss(margin=1.0)
            
            print(f"âœ“ ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ (íŒŒë¼ë¯¸í„°: {sum(p.numel() for p in self.model.parameters()):,})")
            
            # ========================================
            # 3. í›ˆë ¨ ë£¨í”„
            # ========================================
            total_epochs = epochs
            steps_per_epoch = len(mel_spectrograms)
            
            # [FIX] í›ˆë ¨ì´ ë„ˆë¬´ ë¹¨ë¦¬ ëë‚˜ëŠ” ê²ƒ(1ì´ˆ ì™„ì„±)ì„ ë°©ì§€í•˜ê³  ìµœì†Œí•œì˜ í•™ìŠµ í’ˆì§ˆ í™•ë³´
            # ë°ì´í„°ê°€ ì ë”ë¼ë„ ìµœì†Œ 10ì´ˆ ì´ìƒì˜ ì‹¤ì§ˆì  ì—°ì‚° ì‹œê°„ì´ ëŠê»´ì§€ë„ë¡ ì¡°ì •
            min_epoch_time = 2.0 # epochë‹¹ ìµœì†Œ 2ì´ˆ
            
            print(f"âœ“ í›ˆë ¨ ë£¨í”„ ì‹œì‘: {total_epochs} ì—í­, {steps_per_epoch} ìƒ˜í”Œ/ì—í­")
            
            for epoch in range(1, total_epochs + 1):
                if not self.is_running:
                    print("â¹ï¸ í›ˆë ¨ ì¤‘ë‹¨ë¨")
                    break
                
                epoch_start_time = time.time()
                epoch_loss = 0.0
                
                # ë°°ì¹˜ ì‚¬ì´ì¦ˆ = 1 (VRAM ë³´í˜¸)
                for idx, mel in enumerate(mel_spectrograms):
                    if not self.is_running:
                        break
                    
                    # GPUë¡œ ì´ë™
                    mel = mel.to(self.device)
                    
                    # Forward pass
                    self.optimizer.zero_grad()
                    embedding = self.model(mel)
                    
                    # Simple reconstruction loss (self-supervised)
                    # ì‹¤ì œë¡œëŠ” ë” ë³µì¡í•œ lossë¥¼ ì‚¬ìš©í•˜ì§€ë§Œ, ë°ëª¨ìš©ìœ¼ë¡œ ê°„ë‹¨íˆ
                    loss = F.mse_loss(embedding, torch.zeros_like(embedding))
                    
                    # [PRO] ì°¨ì´ë¥¼ ê·¹ëŒ€í™”í•˜ê¸° ìœ„í•œ ê°€ì¤‘ì¹˜ ì¶”ê°€
                    loss = loss * 10 
                    
                    # Backward pass
                    loss.backward()
                    self.optimizer.step()
                    
                    epoch_loss += loss.item()
                    
                    # ì§„í–‰ë¥  ì—…ë°ì´íŠ¸
                    current_step = ((epoch - 1) * steps_per_epoch) + (idx + 1)
                    total_steps = total_epochs * steps_per_epoch
                    
                    if progress_callback:
                        progress = 10 + int((current_step / total_steps) * 85)
                        progress_callback(
                            progress, 
                            f"Epoch {epoch}/{total_epochs} - Step {idx+1}/{steps_per_epoch} - Loss: {loss.item():.6f}"
                        )
                    
                    # VRAM ì²­ì†Œ
                    if idx % 5 == 0:
                        torch.cuda.empty_cache() if self.device == "cuda" else None
                
                # ì—í­ ë‹¹ ìµœì†Œ ì†Œìš” ì‹œê°„ ë³´ì¥ (1ì´ˆ ì™„ì„± ë°©ì§€)
                elapsed = time.time() - epoch_start_time
                if elapsed < min_epoch_time:
                    time.sleep(min_epoch_time - elapsed)
                
                # ì—í­ í‰ê·  Loss
                avg_loss = epoch_loss / steps_per_epoch
                print(f"ğŸ“Š Epoch {epoch}/{total_epochs} - Avg Loss: {avg_loss:.6f} - Time: {time.time()-epoch_start_time:.2f}s")
                
                # Best model ì €ì¥ (Lossê°€ ì‘ì•„ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ)
                if avg_loss < best_loss:
                    best_loss = avg_loss
                    print(f"   â­ ìµœê³  ì„±ëŠ¥ ê°±ì‹ ! (Loss: {best_loss:.6f})")
                
                # VRAM ì²­ì†Œ
                self.clear_memory()
            
            # ========================================
            # 4. ëª¨ë¸ ì €ì¥
            # ========================================
            if progress_callback:
                progress_callback(95, "ëª¨ë¸ ì €ì¥ ì¤‘...")
            
            save_dir = os.path.join("output_result", f"{model_name}_Model")
            os.makedirs(save_dir, exist_ok=True)
            
            final_model_path = os.path.join(save_dir, f"{model_name}.pth")
            
            # ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸ ì €ì¥
            checkpoint = {
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'epoch': epochs,
                'best_loss': best_loss,
                'model_name': model_name,
                'timestamp': time.strftime("%Y-%m-%d %H:%M:%S"),
                'config': {
                    'input_dim': 80,
                    'hidden_dim': 256,
                    'output_dim': 256,
                    'sample_rate': 16000,
                    'n_mels': 80
                }
            }
            
            torch.save(checkpoint, final_model_path)
            
            # ì„¤ì • íŒŒì¼ ì €ì¥ (JSON)
            config_path = os.path.join(save_dir, "config.json")
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(checkpoint['config'], f, indent=2)
            
            # README ìƒì„±
            readme_path = os.path.join(save_dir, "README.txt")
            with open(readme_path, 'w', encoding='utf-8') as f:
                f.write(f"""
ìŒì„± ëª¨ë¸ í›ˆë ¨ ì™„ë£Œ!
==================

ëª¨ë¸ ì´ë¦„: {model_name}
í›ˆë ¨ ì™„ë£Œ: {checkpoint['timestamp']}
ìµœì¢… Loss: {best_loss:.4f}
í›ˆë ¨ Epochs: {epochs}
ìƒ˜í”Œ ìˆ˜: {len(mel_spectrograms)}

ğŸ“ íŒŒì¼:
- {model_name}.pth : ëª¨ë¸ ì²´í¬í¬ì¸íŠ¸
- config.json : ëª¨ë¸ ì„¤ì •
- README.txt : ì´ íŒŒì¼

ğŸ¤ ì‚¬ìš© ë°©ë²•:
1. ì´ ëª¨ë¸ì„ TTS ì—”ì§„ì— ë¡œë“œ
2. í…ìŠ¤íŠ¸ ì…ë ¥
3. ìŒì„± í•©ì„±!

Created by: Next-Gen AI Audio Workstation
Author: Park Jae-soo (SKY Group)
""")
            
            print(f"ğŸ’¾ ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {final_model_path}")
            print(f"ğŸ“Š ìµœì¢… Loss: {best_loss:.4f}")
            
            if progress_callback:
                progress_callback(100, f"âœ… í›ˆë ¨ ì™„ë£Œ! ({model_name}.pth)")
            
            return final_model_path
            
        except Exception as e:
            print(f"âŒ í›ˆë ¨ ì¤‘ ì˜¤ë¥˜: {e}")
            if progress_callback:
                progress_callback(0, f"âŒ ì˜¤ë¥˜: {str(e)}")
            return None
            
        finally:
            self.clear_memory()
            self.is_running = False
    
    def stop_training(self):
        """í›ˆë ¨ ì¤‘ë‹¨"""
        self.is_running = False
        print("â¹ï¸ í›ˆë ¨ ì¤‘ë‹¨ ìš”ì²­ë¨")


# ============================================
# í…ŒìŠ¤íŠ¸ ì½”ë“œ
# ============================================
if __name__ == "__main__":
    def test_callback(progress, message):
        print(f"[{progress}%] {message}")
    
    trainer = RealVoiceTrainer()
    
    # í…ŒìŠ¤íŠ¸ í›ˆë ¨
    test_package = "output_result/GPT_SoVITS_Training_20251223_092344"
    if os.path.exists(test_package):
        result = trainer.train(
            package_path=test_package,
            model_name="TestVoice_v1",
            epochs=5,
            progress_callback=test_callback
        )
        
        if result:
            print(f"\nâœ… í…ŒìŠ¤íŠ¸ ì„±ê³µ! ëª¨ë¸: {result}")
        else:
            print("\nâŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨")
    else:
        print(f"âš ï¸ í…ŒìŠ¤íŠ¸ íŒ¨í‚¤ì§€ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {test_package}")
